# Tutorial MLFlow - Exemplos Pr√°ticos

## Vis√£o Geral

Este tutorial pr√°tico guia voc√™ atrav√©s de quatro exemplos progressivos que demonstram as principais funcionalidades do MLFlow.

## Prepara√ß√£o

Antes de come√ßar, certifique-se de:

1. ‚úÖ Ter instalado todas as depend√™ncias (`pip install -r requirements.txt`)
2. ‚úÖ Ter gerado os dados (`python data/generate_data.py`)
3. ‚úÖ Estar na raiz do projeto

## Exemplo 1: Tracking B√°sico

### Objetivo

Aprender o b√°sico do MLFlow tracking: logar par√¢metros, m√©tricas e salvar modelos.

### O que voc√™ vai aprender

- Criar e configurar um experimento
- Iniciar uma run
- Logar par√¢metros do modelo
- Logar m√©tricas de avalia√ß√£o
- Salvar o modelo treinado

### Executar

```bash
cd models
python 01_basic_tracking.py
```

### O que acontece

1. **Carrega os dados** de churn de clientes do CSV
2. **Divide** em treino (80%) e teste (20%)
3. **Treina** um modelo de Regress√£o Log√≠stica
4. **Loga no MLFlow**:
   - Par√¢metros: `C`, `solver`, `max_iter`
   - M√©tricas: `accuracy`, `precision`, `recall`, `f1_score`
   - Informa√ß√µes adicionais: quantidade de amostras, features
   - Modelo treinado

### Sa√≠da esperada

```
==============================================================
EXEMPLO 1: TRACKING B√ÅSICO COM MLFLOW
==============================================================

üìä Experimento: Churn_Prediction_Basic

üìÅ Carregando dados...
   - Total de registros: 1000
   - Treino: 800 | Teste: 200
   - Features: ['idade', 'tempo_cliente_meses', ...]

üîß Par√¢metros do modelo: {'C': 1.0, 'solver': 'lbfgs', 'max_iter': 100}

üöÄ Treinando modelo...
üìà Avaliando modelo...

‚úì Treinamento conclu√≠do!

üìä M√©tricas:
   - accuracy: 0.7850
   - precision: 0.7234
   - recall: 0.6891
   - f1_score: 0.7058

üîó Run ID: a7b3c4d5e6f7g8h9...
```

### Visualizar na UI

1. Execute `mlflow ui` (em outro terminal)
2. Acesse `http://localhost:5000`
3. Clique no experimento "Churn_Prediction_Basic"
4. Veja os par√¢metros, m√©tricas e artefatos da run

### Pontos de aten√ß√£o

- **Run Name**: Nome descritivo ajuda a identificar runs espec√≠ficas
- **Par√¢metros vs M√©tricas**: Par√¢metros s√£o inputs, m√©tricas s√£o outputs
- **Modelo Salvo**: O modelo fica dispon√≠vel em `mlruns/.../artifacts/model`

---

## Exemplo 2: Logging de Artefatos

### Objetivo

Aprender a logar artefatos visuais e comparar diferentes modelos.

### O que voc√™ vai aprender

- Logar gr√°ficos (confusion matrix, ROC curve)
- Logar feature importance
- Salvar predi√ß√µes em CSV
- Comparar m√∫ltiplos modelos

### Executar

```bash
python 02_artifacts_tracking.py
```

### O que acontece

1. **Treina dois modelos**:
   - Regress√£o Log√≠stica
   - √Årvore de Decis√£o
2. **Para cada modelo, loga**:
   - Confusion Matrix (heatmap)
   - Curva ROC
   - Feature Importance
   - Arquivo CSV com predi√ß√µes
3. **Compara** os resultados dos dois modelos

### Sa√≠da esperada

```
==============================================================
EXEMPLO 2: LOGGING DE ARTEFATOS
==============================================================

==============================================================
ü§ñ Modelo: Logistic Regression
==============================================================

üöÄ Treinando...

üìä M√©tricas:
   - accuracy: 0.7850
   - precision: 0.7234
   - recall: 0.6891
   - f1_score: 0.7058
   - roc_auc: 0.8234

üìà Gerando artefatos...
‚úì Artefatos salvos com sucesso!
   - Confusion Matrix
   - ROC Curve
   - Feature Importance
   - Predictions CSV

==============================================================
ü§ñ Modelo: Decision Tree
==============================================================
[similar output]

==============================================================
üìä COMPARA√á√ÉO DE MODELOS
==============================================================
                       accuracy  precision    recall  f1_score   roc_auc
Logistic Regression    0.7850    0.7234      0.6891    0.7058    0.8234
Decision Tree          0.7650    0.6982      0.7123    0.7051    0.7891

üèÜ Melhor modelo (F1-Score): Logistic Regression
```

### Visualizar na UI

1. Abra `http://localhost:5000`
2. Clique no experimento "Churn_Prediction_Artifacts"
3. Selecione uma run
4. V√° para a aba "Artifacts"
5. Visualize os gr√°ficos clicando neles

### Pontos de aten√ß√£o

- **Artefatos Visuais**: Ajudam a entender o desempenho do modelo
- **Compara√ß√£o**: A tabela final facilita identificar o melhor modelo
- **CSV de Predi√ß√µes**: √ötil para an√°lise posterior ou auditoria

---

## Exemplo 3: Compara√ß√£o de M√∫ltiplos Experimentos

### Objetivo

Executar grid search e comparar dezenas de modelos automaticamente.

### O que voc√™ vai aprender

- Executar m√∫ltiplas runs em loop
- Grid search de hiperpar√¢metros
- Comparar modelos diferentes (Logistic Regression, Decision Tree, Random Forest)
- Identificar automaticamente o melhor modelo

### Executar

```bash
python 03_compare_experiments.py
```

‚ö†Ô∏è **Aten√ß√£o**: Este script executa muitas runs (6 LR + 12 DT + 8 RF = 26 runs) e pode levar alguns minutos.

### O que acontece

1. **Regress√£o Log√≠stica**: Testa 6 combina√ß√µes de hiperpar√¢metros
   - `C`: [0.1, 1.0, 10.0]
   - `solver`: ['lbfgs', 'liblinear']

2. **√Årvore de Decis√£o**: Testa 12 combina√ß√µes
   - `max_depth`: [3, 5, 7, 10]
   - `min_samples_split`: [2, 10, 20]

3. **Random Forest**: Testa 8 combina√ß√µes
   - `n_estimators`: [50, 100]
   - `max_depth`: [5, 10]
   - `min_samples_split`: [2, 10]

4. **Compara** todos os resultados e identifica o melhor

### Sa√≠da esperada

```
======================================================================
EXEMPLO 3: COMPARA√á√ÉO DE M√öLTIPLOS EXPERIMENTOS
======================================================================

======================================================================
üîµ REGRESS√ÉO LOG√çSTICA - Grid Search
======================================================================
Total de combina√ß√µes: 6

[1/6] Testando: {'C': 0.1, 'solver': 'lbfgs', 'max_iter': 100}
   F1-Score: 0.7012 | ROC-AUC: 0.8145
[2/6] Testando: {'C': 1.0, 'solver': 'lbfgs', 'max_iter': 100}
   F1-Score: 0.7058 | ROC-AUC: 0.8234
...

======================================================================
üìä RESUMO DE TODOS OS EXPERIMENTOS
======================================================================

üèÜ TOP 10 MODELOS (ordenados por F1-Score):

model_type          f1_score  roc_auc  accuracy  run_id
Random Forest       0.7234    0.8456   0.7950    abc123...
Random Forest       0.7198    0.8423   0.7925    def456...
Decision Tree       0.7156    0.8134   0.7850    ghi789...
...

ü•á MELHOR MODELO POR TIPO:

model_type          f1_score  roc_auc  params
Random Forest       0.7234    0.8456    {'n_estimators': 100, 'max_depth': 10, ...}
Decision Tree       0.7156    0.8134    {'max_depth': 7, 'min_samples_split': 2}
Logistic Regression 0.7058    0.8234    {'C': 1.0, 'solver': 'lbfgs', ...}

======================================================================
üéØ MELHOR MODELO GERAL
======================================================================
Tipo: Random Forest
Par√¢metros: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2}
F1-Score: 0.7234
ROC-AUC: 0.8456
Accuracy: 0.7950
Run ID: abc123...

‚úì Resultados salvos em: comparison_results.csv
```

### Visualizar na UI

1. Abra `http://localhost:5000`
2. Clique no experimento "Churn_Prediction_Comparison"
3. **Compare runs**:
   - Selecione m√∫ltiplas runs (checkbox)
   - Clique em "Compare"
   - Visualize tabela comparativa e gr√°ficos

### Dicas de visualiza√ß√£o

- **Filtrar por modelo**: Use a barra de busca com `tags.model_type = "Random Forest"`
- **Ordenar**: Clique nos headers das colunas para ordenar por m√©trica
- **Gr√°fico de compara√ß√£o**: Visualize tend√™ncias de hiperpar√¢metros vs m√©tricas

### Pontos de aten√ß√£o

- **Nomea√ß√£o de Runs**: Cada run tem nome √∫nico com os par√¢metros
- **CSV de Resultados**: Salvo para an√°lise offline em `comparison_results.csv`
- **Escalabilidade**: Para grids grandes, considere usar ferramentas de otimiza√ß√£o (Optuna, Hyperopt)

---

## Exemplo 4: Model Registry

### Objetivo

Aprender a usar o Model Registry para gerenciar vers√µes e lifecycle de modelos.

### O que voc√™ vai aprender

- Registrar modelos no Model Registry
- Criar m√∫ltiplas vers√µes
- Transicionar entre stages (Staging, Production, Archived)
- Carregar modelos registrados
- Adicionar descri√ß√µes e tags

### Executar

```bash
python 04_model_registry.py
```

### O que acontece

1. **Treina Vers√£o 1**:
   - Random Forest baseline
   - Registra no Model Registry
   - Move para stage "Staging"

2. **Treina Vers√£o 2**:
   - Random Forest otimizado
   - Registra como nova vers√£o
   - Move para stage "Production"
   - Arquiva vers√£o anterior

3. **Gerencia o registro**:
   - Adiciona descri√ß√µes
   - Adiciona tags
   - Lista todas as vers√µes

4. **Carrega modelo de produ√ß√£o**:
   - Usa URI especial `models:/{nome}/Production`
   - Faz predi√ß√µes de exemplo

### Sa√≠da esperada

```
======================================================================
EXEMPLO 4: MODEL REGISTRY - GERENCIAMENTO DE MODELOS
======================================================================

üìä Experimento: Churn_Prediction_Registry
üè∑Ô∏è  Nome do modelo: churn_prediction_model

======================================================================
üì¶ VERS√ÉO 1: Modelo Inicial
======================================================================

üöÄ Treinando novo modelo: Vers√£o 1 - Modelo baseline
   M√©tricas - F1: 0.7145, ROC-AUC: 0.8312

‚úì Modelo registrado como vers√£o 1
‚úì Vers√£o 1 movida para: Staging

======================================================================
üì¶ VERS√ÉO 2: Modelo Melhorado
======================================================================

üöÄ Treinando novo modelo: Vers√£o 2 - Modelo otimizado
   M√©tricas - F1: 0.7234, ROC-AUC: 0.8456

‚úì Modelo registrado como vers√£o 2
‚úì Vers√£o 2 movida para: Production
‚úì Vers√£o 1 movida para: Archived

======================================================================
üìã TODAS AS VERS√ïES DO MODELO
======================================================================

Vers√£o  Stage       Run ID       Criado em
2       Production  abc123...    2024-01-15 10:30
1       Archived    def456...    2024-01-15 10:29

======================================================================
üîÑ CARREGANDO MODELO DE PRODU√á√ÉO
======================================================================

‚úì Modelo carregado: models:/churn_prediction_model/Production

üìä Exemplo de predi√ß√µes com modelo de produ√ß√£o:
   idade  tempo_cliente_meses  ...  Predi√ß√£o  Probabilidade Churn
   35     24                  ...  0         0.234
   52     8                   ...  1         0.789
   ...
```

### Visualizar na UI

1. Abra `http://localhost:5000`
2. Clique na aba **"Models"** (topo da p√°gina)
3. Veja o modelo "churn_prediction_model"
4. Explore:
   - Vers√µes do modelo
   - Stage atual de cada vers√£o
   - Descri√ß√µes e tags
   - M√©tricas linkadas

### Opera√ß√µes no Model Registry (via UI)

- **Transi√ß√£o de Stage**: Clique em "Stage" ‚Üí Selecione novo stage
- **Adicionar Descri√ß√£o**: Edite a descri√ß√£o da vers√£o
- **Comparar Vers√µes**: Selecione m√∫ltiplas vers√µes e compare
- **Ver Run Original**: Clique no link da run para ver detalhes

### Pontos de aten√ß√£o

- **Stages**: None ‚Üí Staging ‚Üí Production ‚Üí Archived
- **M√∫ltiplas Vers√µes em Production**: Poss√≠vel ter > 1 vers√£o em Production
- **Carregar Modelo**: Use `models:/{nome}/{stage}` ou `models:/{nome}/{version}`
- **Auditoria**: Todas as transi√ß√µes ficam registradas

---

## Comandos √öteis

### Executar todos os exemplos em sequ√™ncia

```bash
cd models
python 01_basic_tracking.py && \
python 02_artifacts_tracking.py && \
python 03_compare_experiments.py && \
python 04_model_registry.py
```

### Limpar dados do MLFlow

```bash
# ‚ö†Ô∏è CUIDADO: Remove todos os experimentos
rm -rf mlruns/
rm -rf mlartifacts/
```

### Exportar experimento

```bash
mlflow experiments csv -x 1 -o experiment_1.csv
```

### Ver informa√ß√µes de uma run espec√≠fica

```python
from mlflow.tracking import MlflowClient

client = MlflowClient()
run = client.get_run("run_id_aqui")
print(run.data.params)
print(run.data.metrics)
```

## Exerc√≠cios Pr√°ticos

### Exerc√≠cio 1: Modificar Hiperpar√¢metros

Edite `01_basic_tracking.py` e teste diferentes valores de `C`:

```python
params = {
    'C': 0.5,  # Teste: 0.1, 0.5, 2.0, 10.0
    'solver': 'lbfgs',
    'max_iter': 100
}
```

Compare as m√©tricas na UI.

### Exerc√≠cio 2: Adicionar Nova M√©trica

No `02_artifacts_tracking.py`, adicione a m√©trica de especificidade:

```python
from sklearn.metrics import confusion_matrix

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
specificity = tn / (tn + fp)
mlflow.log_metric("specificity", specificity)
```

### Exerc√≠cio 3: Novo Tipo de Modelo

No `03_compare_experiments.py`, adicione o SVM:

```python
from sklearn.svm import SVC

svm_param_grid = {
    'C': [0.1, 1.0, 10.0],
    'kernel': ['linear', 'rbf']
}
# Implemente o loop similar aos outros modelos
```

### Exerc√≠cio 4: Promover Modelo

Crie um script que:
1. Busca a run com melhor F1-score
2. Registra esse modelo no Registry
3. Promove para Production automaticamente

## Pr√≥ximos Passos

Parab√©ns! Voc√™ completou os exemplos pr√°ticos. Continue para:

- **[05 - Model Registry](05-model-registry.md)**: Aprofunde no gerenciamento de modelos
- **[06 - Melhores Pr√°ticas](06-melhores-praticas.md)**: Aprenda dicas avan√ßadas

---

**Dica**: Experimente modificar os c√≥digos e ver o impacto nas m√©tricas. O MLFlow torna f√°cil experimentar e comparar!
