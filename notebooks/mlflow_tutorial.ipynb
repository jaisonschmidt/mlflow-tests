{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c79ed8fa",
   "metadata": {},
   "source": [
    "## 1. Setup e ImportaÃ§Ãµes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImportaÃ§Ãµes bÃ¡sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MLFlow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc, roc_auc_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# ConfiguraÃ§Ãµes\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… ImportaÃ§Ãµes concluÃ­das\")\n",
    "print(f\"MLFlow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f7fe4",
   "metadata": {},
   "source": [
    "## 2. Carregar e Explorar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76866d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "data_path = Path('../data/customer_churn.csv')\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(\"âš ï¸ Dados nÃ£o encontrados. Execute: python data/generate_data.py\")\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"âœ… Dados carregados com sucesso!\")\n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "    print(f\"\\nColunas: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01099d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar primeiras linhas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a3acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EstatÃ­sticas descritivas\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistribuiÃ§Ã£o da variÃ¡vel target\n",
    "churn_counts = df['churn'].value_counts()\n",
    "churn_pct = df['churn'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"DistribuiÃ§Ã£o de Churn:\")\n",
    "print(f\"  Sem churn (0): {churn_counts[0]} ({churn_pct[0]:.1f}%)\")\n",
    "print(f\"  Com churn (1): {churn_counts[1]} ({churn_pct[1]:.1f}%)\")\n",
    "\n",
    "# GrÃ¡fico\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "churn_counts.plot(kind='bar', ax=ax, color=['green', 'red'])\n",
    "ax.set_xlabel('Churn')\n",
    "ax.set_ylabel('Quantidade')\n",
    "ax.set_title('DistribuiÃ§Ã£o de Churn')\n",
    "ax.set_xticklabels(['Sem Churn', 'Com Churn'], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b9133",
   "metadata": {},
   "source": [
    "## 3. Preparar Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc58af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features e target\n",
    "X = df.drop(['cliente_id', 'churn'], axis=1)\n",
    "y = df['churn']\n",
    "\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "print(f\"\\nShape de X: {X.shape}\")\n",
    "print(f\"Shape de y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2079fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Treino: {len(X_train)} amostras\")\n",
    "print(f\"Teste: {len(X_test)} amostras\")\n",
    "print(f\"\\nDistribuiÃ§Ã£o de churn no treino: {y_train.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"DistribuiÃ§Ã£o de churn no teste: {y_test.value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38da2a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Exemplo 1: Tracking BÃ¡sico com MLFlow\n",
    "\n",
    "Vamos treinar um modelo de RegressÃ£o LogÃ­stica e usar o MLFlow para rastrear:\n",
    "- ParÃ¢metros do modelo\n",
    "- MÃ©tricas de desempenho\n",
    "- O modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dbdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar experimento\n",
    "experiment_name = \"Notebook_Churn_Prediction\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"âœ… Experimento configurado: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ffd6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo com tracking\n",
    "with mlflow.start_run(run_name=\"logistic_regression_baseline\"):\n",
    "    \n",
    "    # ParÃ¢metros\n",
    "    params = {\n",
    "        'C': 1.0,\n",
    "        'solver': 'lbfgs',\n",
    "        'max_iter': 100,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸš€ Treinando RegressÃ£o LogÃ­stica...\")\n",
    "    \n",
    "    # Treinar\n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # PrediÃ§Ãµes\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # MÃ©tricas\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    # Logar no MLFlow\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.log_param(\"train_samples\", len(X_train))\n",
    "    mlflow.log_param(\"test_samples\", len(X_test))\n",
    "    \n",
    "    # Salvar modelo\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # Tags\n",
    "    mlflow.set_tag(\"model_type\", \"Logistic Regression\")\n",
    "    mlflow.set_tag(\"notebook\", \"tutorial\")\n",
    "    \n",
    "    print(\"\\nâœ… Treinamento concluÃ­do!\")\n",
    "    print(\"\\nðŸ“Š MÃ©tricas:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"   {name}: {value:.4f}\")\n",
    "    \n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"\\nðŸ”— Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f532c3",
   "metadata": {},
   "source": [
    "ðŸ’¡ **Dica**: Abra o MLFlow UI em outro terminal com `mlflow ui` e acesse `http://localhost:5000` para ver os resultados!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb0673",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Exemplo 2: Logging de Artefatos\n",
    "\n",
    "Vamos adicionar visualizaÃ§Ãµes e salvÃ¡-las como artefatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"logistic_regression_with_artifacts\"):\n",
    "    \n",
    "    # Treinar modelo\n",
    "    params = {'C': 1.0, 'solver': 'lbfgs', 'max_iter': 100, 'random_state': 42}\n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # PrediÃ§Ãµes\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # MÃ©tricas\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    # Logar parÃ¢metros e mÃ©tricas\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # === ARTEFATO 1: Confusion Matrix ===\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predito')\n",
    "    ax.set_ylabel('Real')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    mlflow.log_figure(fig, \"confusion_matrix.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # === ARTEFATO 2: ROC Curve ===\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('Taxa de Falsos Positivos')\n",
    "    ax.set_ylabel('Taxa de Verdadeiros Positivos')\n",
    "    ax.set_title('Curva ROC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    mlflow.log_figure(fig, \"roc_curve.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # === ARTEFATO 3: Feature Importance ===\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': np.abs(model.coef_[0])\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "    ax.set_xlabel('ImportÃ¢ncia (|coeficiente|)')\n",
    "    ax.set_title('Feature Importance')\n",
    "    mlflow.log_figure(fig, \"feature_importance.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Salvar modelo\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(\"\\nâœ… Modelo treinado e artefatos salvos!\")\n",
    "    print(f\"   - Confusion Matrix\")\n",
    "    print(f\"   - ROC Curve\")\n",
    "    print(f\"   - Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45066d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Exemplo 3: ComparaÃ§Ã£o de Modelos\n",
    "\n",
    "Vamos treinar 3 modelos diferentes e comparar os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102bb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunÃ§Ã£o para treinar e avaliar modelo\n",
    "def train_and_evaluate(model, model_name, params):\n",
    "    \"\"\"Treina modelo e loga no MLFlow.\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Treinar\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # PrediÃ§Ãµes\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # MÃ©tricas\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "        \n",
    "        # Logar\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.set_tag(\"model_type\", model_name)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "print(\"FunÃ§Ã£o de treinamento definida âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca831a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 1: RegressÃ£o LogÃ­stica\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ”µ Modelo 1: RegressÃ£o LogÃ­stica\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lr_model = LogisticRegression(C=1.0, max_iter=100, random_state=42)\n",
    "lr_params = {'C': 1.0, 'max_iter': 100, 'random_state': 42}\n",
    "lr_metrics = train_and_evaluate(lr_model, \"Logistic_Regression\", lr_params)\n",
    "\n",
    "print(\"MÃ©tricas:\")\n",
    "for name, value in lr_metrics.items():\n",
    "    print(f\"  {name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 2: Ãrvore de DecisÃ£o\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŒ³ Modelo 2: Ãrvore de DecisÃ£o\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_params = {'max_depth': 5, 'random_state': 42}\n",
    "dt_metrics = train_and_evaluate(dt_model, \"Decision_Tree\", dt_params)\n",
    "\n",
    "print(\"MÃ©tricas:\")\n",
    "for name, value in dt_metrics.items():\n",
    "    print(f\"  {name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 3: Random Forest\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŒ² Modelo 3: Random Forest\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_params = {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}\n",
    "rf_metrics = train_and_evaluate(rf_model, \"Random_Forest\", rf_params)\n",
    "\n",
    "print(\"MÃ©tricas:\")\n",
    "for name, value in rf_metrics.items():\n",
    "    print(f\"  {name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar resultados\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Logistic Regression': lr_metrics,\n",
    "    'Decision Tree': dt_metrics,\n",
    "    'Random Forest': rf_metrics\n",
    "}).T\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š COMPARAÃ‡ÃƒO DE MODELOS\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "# Melhor modelo\n",
    "best_model = comparison_df['f1_score'].idxmax()\n",
    "print(f\"\\nðŸ† Melhor modelo (F1-Score): {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3bf06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparaÃ§Ã£o\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# GrÃ¡fico 1: Todas as mÃ©tricas\n",
    "comparison_df.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('ComparaÃ§Ã£o de MÃ©tricas por Modelo')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].legend(title='Modelo', bbox_to_anchor=(1.05, 1))\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# GrÃ¡fico 2: F1-Score\n",
    "comparison_df['f1_score'].plot(kind='barh', ax=axes[1], color=['blue', 'green', 'orange'])\n",
    "axes[1].set_title('F1-Score por Modelo')\n",
    "axes[1].set_xlabel('F1-Score')\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d524dcc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Buscar e Analisar Runs Programaticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f474dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter experimento atual\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "print(f\"Experimento: {experiment.name}\")\n",
    "print(f\"ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar todas as runs do experimento\n",
    "client = MlflowClient()\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    order_by=[\"metrics.f1_score DESC\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal de runs encontradas: {len(runs)}\\n\")\n",
    "\n",
    "# Exibir top 5 runs\n",
    "print(\"ðŸ† Top 5 Runs (ordenado por F1-Score):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, run in enumerate(runs[:5], 1):\n",
    "    run_name = run.data.tags.get('mlflow.runName', 'N/A')\n",
    "    f1 = run.data.metrics.get('f1_score', 0)\n",
    "    accuracy = run.data.metrics.get('accuracy', 0)\n",
    "    \n",
    "    print(f\"{i}. {run_name}\")\n",
    "    print(f\"   F1-Score: {f1:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter runs para DataFrame para anÃ¡lise\n",
    "runs_data = []\n",
    "for run in runs:\n",
    "    data = {\n",
    "        'run_id': run.info.run_id,\n",
    "        'run_name': run.data.tags.get('mlflow.runName', 'N/A'),\n",
    "        'model_type': run.data.tags.get('model_type', 'N/A'),\n",
    "        **run.data.metrics\n",
    "    }\n",
    "    runs_data.append(data)\n",
    "\n",
    "runs_df = pd.DataFrame(runs_data)\n",
    "print(\"\\nDataFrame de Runs:\")\n",
    "runs_df[['run_name', 'model_type', 'f1_score', 'accuracy', 'roc_auc']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79466689",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Model Registry (Opcional)\n",
    "\n",
    "Registre o melhor modelo no Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4774b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome do modelo no registry\n",
    "model_name = \"churn_predictor_notebook\"\n",
    "\n",
    "# Treinar e registrar melhor modelo\n",
    "with mlflow.start_run(run_name=\"best_model_for_registry\"):\n",
    "    \n",
    "    # Usar Random Forest (melhor modelo)\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Avaliar\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    # Logar\n",
    "    mlflow.log_params({'n_estimators': 100, 'max_depth': 10})\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Registrar no Model Registry\n",
    "    mlflow.sklearn.log_model(\n",
    "        model,\n",
    "        \"model\",\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Modelo registrado no Model Registry: {model_name}\")\n",
    "    print(f\"\\nMÃ©tricas:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar versÃµes do modelo\n",
    "import time\n",
    "time.sleep(2)  # Aguardar registro completar\n",
    "\n",
    "try:\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    \n",
    "    print(f\"\\nVersÃµes do modelo '{model_name}':\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for version in versions:\n",
    "        print(f\"VersÃ£o: {version.version}\")\n",
    "        print(f\"  Stage: {version.current_stage}\")\n",
    "        print(f\"  Run ID: {version.run_id}\")\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print(f\"Modelo ainda nÃ£o registrado ou erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b22d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Carregar Modelo do MLFlow\n",
    "\n",
    "Demonstre como carregar e usar um modelo salvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ff50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter Run ID do melhor modelo\n",
    "best_run = runs[0]  # Primeiro run (ordenado por f1_score DESC)\n",
    "best_run_id = best_run.info.run_id\n",
    "\n",
    "print(f\"Melhor Run ID: {best_run_id}\")\n",
    "print(f\"F1-Score: {best_run.data.metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "print(f\"âœ… Modelo carregado de: {model_uri}\")\n",
    "print(f\"Tipo: {type(loaded_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1564f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer prediÃ§Ãµes com modelo carregado\n",
    "sample_data = X_test.head(10)\n",
    "predictions = loaded_model.predict(sample_data)\n",
    "probabilities = loaded_model.predict_proba(sample_data)[:, 1]\n",
    "\n",
    "# Exibir resultados\n",
    "results = sample_data.copy()\n",
    "results['Real'] = y_test.head(10).values\n",
    "results['PrediÃ§Ã£o'] = predictions\n",
    "results['Prob. Churn'] = probabilities.round(3)\n",
    "\n",
    "print(\"\\nðŸ“Š PrediÃ§Ãµes de Exemplo:\")\n",
    "print(results[['idade', 'tempo_cliente_meses', 'satisfacao', 'Real', 'PrediÃ§Ã£o', 'Prob. Churn']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe4ecd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. ExercÃ­cios PrÃ¡ticos\n",
    "\n",
    "### ExercÃ­cio 1: Testar Diferentes HiperparÃ¢metros\n",
    "\n",
    "Teste diferentes valores de `C` para RegressÃ£o LogÃ­stica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb1b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu cÃ³digo aqui\n",
    "# Dica: Use um loop para testar C = [0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "# e logue cada run no MLFlow\n",
    "\n",
    "# Exemplo de estrutura:\n",
    "# for C_value in [0.1, 0.5, 1.0, 5.0, 10.0]:\n",
    "#     with mlflow.start_run(run_name=f\"LR_C_{C_value}\"):\n",
    "#         # Treinar modelo\n",
    "#         # Logar parÃ¢metros e mÃ©tricas\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd00db",
   "metadata": {},
   "source": [
    "### ExercÃ­cio 2: Adicionar Nova MÃ©trica\n",
    "\n",
    "Calcule e logue a mÃ©trica de **especificidade**:\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15874ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu cÃ³digo aqui\n",
    "# Dica: Use confusion_matrix para obter TN e FP\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "# specificity = tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9528ea6",
   "metadata": {},
   "source": [
    "### ExercÃ­cio 3: Buscar Runs com Filtro\n",
    "\n",
    "Busque apenas runs com `accuracy > 0.75`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084dd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu cÃ³digo aqui\n",
    "# Dica: Use client.search_runs() com filter_string\n",
    "# filtered_runs = client.search_runs(\n",
    "#     experiment_ids=[experiment_id],\n",
    "#     filter_string=\"metrics.accuracy > 0.75\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e43fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Resumo e PrÃ³ximos Passos\n",
    "\n",
    "### O que vocÃª aprendeu:\n",
    "\n",
    "âœ… Conceitos bÃ¡sicos do MLFlow (Experiments, Runs, Parameters, Metrics)\n",
    "âœ… Como fazer tracking de modelos de ML\n",
    "âœ… Como logar artefatos (grÃ¡ficos, modelos)\n",
    "âœ… Como comparar mÃºltiplos experimentos\n",
    "âœ… Como usar o Model Registry\n",
    "âœ… Como carregar modelos salvos\n",
    "âœ… Como analisar runs programaticamente\n",
    "\n",
    "### PrÃ³ximos Passos:\n",
    "\n",
    "1. **Explore a UI do MLFlow**: Execute `mlflow ui` e visualize todos os experimentos\n",
    "2. **Execute os scripts Python**: Teste os exemplos em `models/01_*.py`\n",
    "3. **Leia a documentaÃ§Ã£o**: Veja `tutorial/` para guias detalhados\n",
    "4. **Experimente**: Teste diferentes modelos e hiperparÃ¢metros\n",
    "5. **Integre em seus projetos**: Use MLFlow em seus prÃ³prios projetos de ML\n",
    "\n",
    "---\n",
    "\n",
    "### Recursos Adicionais:\n",
    "\n",
    "- [DocumentaÃ§Ã£o oficial do MLFlow](https://mlflow.org/docs/latest/index.html)\n",
    "- [MLFlow GitHub](https://github.com/mlflow/mlflow)\n",
    "- [Tutorial completo em `tutorial/`](../tutorial/)\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ ParabÃ©ns por completar o tutorial!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
